{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from transformers import ViTImageProcessor\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_pytorch(model, test_loader, categories):\n",
    "    device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            images = images.squeeze(1)  # Shape becomes [32, 3, 224, 224]\n",
    "\n",
    "            outputs = model(images) \n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # If labels are one-hot encoded, convert them to class indices\n",
    "            if len(labels.shape) > 1:\n",
    "                labels = labels.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Print Classification Report\n",
    "    print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, target_names=categories))\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_thresholds_pytorch(model, test_loader, num_classes, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            labels = labels.cpu().numpy()\n",
    "            if labels.ndim == 1:\n",
    "                labels = labels.reshape(-1, 1)  # Shape [batch_size, 1]\n",
    "\n",
    "            images = images.squeeze(1)  # Shape becomes [batch, 3, 224, 224]\n",
    "\n",
    "            # Get model output logits\n",
    "            outputs = model(images) \n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()  # Softmax for multi-class classification\n",
    "\n",
    "            all_probs.append(probs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)  # Shape: [n_samples, n_classes]\n",
    "    all_labels = np.concatenate(all_labels, axis=0)  # Shape: [n_samples, 1] \n",
    "\n",
    "    # One-hot encode labels (needed for multi-class precision-recall curve)\n",
    "    all_labels = label_binarize(all_labels, classes=np.arange(num_classes))  # Shape: [n_samples, n_classes]\n",
    "\n",
    "    best_thresholds = []\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        binary_labels = all_labels[:, class_idx]  # Get class labels (binary format)\n",
    "\n",
    "        # Compute precision-recall curve\n",
    "        precision, recall, thresholds = precision_recall_curve(binary_labels, all_probs[:, class_idx])\n",
    "\n",
    "        # Compute F1 scores for each threshold\n",
    "        f1_scores = (2 * precision * recall) / (precision + recall + np.finfo(float).eps)  # Avoid division by zero\n",
    "\n",
    "        # Get threshold with max F1 score\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)] if thresholds.size > 0 else 0.5\n",
    "        best_thresholds.append(best_threshold)\n",
    "\n",
    "        print(f\"Best threshold for class {class_idx}: {best_threshold:.4f}\")\n",
    "\n",
    "    return best_thresholds\n",
    "\n",
    "def evaluate_with_best_thresholds_pytorch(model, test_loader, categories, device):\n",
    "    best_thresholds = find_best_thresholds_pytorch(model, test_loader, len(categories), device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            if len(images.shape) == 5:\n",
    "                images = images.squeeze(1)  # Remove the extra dimension, if necessary\n",
    "\n",
    "            # Ensure labels are in a multi-label format (one-hot encoded)\n",
    "            if labels.ndim == 1: \n",
    "                labels = np.eye(len(categories))[labels]\n",
    "\n",
    "            outputs = model(images)\n",
    "            if isinstance(outputs, tuple):  \n",
    "                outputs = outputs[0]\n",
    "\n",
    "            # Apply sigmoid activation\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "            # Apply best thresholds for each class\n",
    "            preds = (probs > np.array(best_thresholds)).astype(int)\n",
    "            preds = np.argmax(preds*probs, axis=1)\n",
    "            preds = label_binarize(preds, classes=np.arange(len(categories)))\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Ensure both are in the same shape and format\n",
    "    if all_labels.shape != all_preds.shape:\n",
    "        raise ValueError(f\"Shape mismatch: labels {all_labels.shape}, preds {all_preds.shape}\")\n",
    "    \n",
    "    all_labels = all_labels.argmax(axis=1)\n",
    "    all_preds = all_preds.argmax(axis=1)\n",
    "\n",
    "    # Print Classification Report\n",
    "    print(\"Classification Report with Optimized Thresholds:\\n\",\n",
    "          classification_report(all_labels, all_preds, target_names=categories))\n",
    "    \n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "categories = [\"Normal\",\"Osteopenia\", \"Osteoporosis\"]\n",
    "\n",
    "root_dir = 'Dataset'\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "val_dir = os.path.join(root_dir, 'val')\n",
    "test_dir = os.path.join(root_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, labels, processor, model, device):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.device = device \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        inputs = self.processor(images=image, return_tensors=\"np\").to(self.device)  # Move inputs to device\n",
    "        inputs = inputs.pixel_values \n",
    "\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 3576\n",
      "Validation images: 1038\n",
      "Test images: 536\n"
     ]
    }
   ],
   "source": [
    "train_paths = []\n",
    "train_labels = []\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(os.listdir(train_dir))}\n",
    "for class_name in os.listdir(train_dir):\n",
    "    class_folder = os.path.join(train_dir, class_name)\n",
    "    if os.path.isdir(class_folder):\n",
    "        for img_name in os.listdir(class_folder):\n",
    "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                train_paths.append(os.path.join(class_folder, img_name))\n",
    "                train_labels.append(class_to_idx[class_name])\n",
    "print(f\"Training images: {len(train_paths)}\")\n",
    "\n",
    "val_paths = []\n",
    "val_labels = []\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(os.listdir(val_dir))}\n",
    "for class_name in os.listdir(val_dir):\n",
    "    class_folder = os.path.join(val_dir, class_name)\n",
    "    if os.path.isdir(class_folder):\n",
    "        for img_name in os.listdir(class_folder):\n",
    "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                val_paths.append(os.path.join(class_folder, img_name))\n",
    "                val_labels.append(class_to_idx[class_name])\n",
    "print(f\"Validation images: {len(val_paths)}\")\n",
    "\n",
    "test_paths = []\n",
    "test_labels = []\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(os.listdir(test_dir))}\n",
    "for class_name in os.listdir(test_dir):\n",
    "    class_folder = os.path.join(test_dir, class_name)\n",
    "    if os.path.isdir(class_folder):\n",
    "        for img_name in os.listdir(class_folder):\n",
    "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                test_paths.append(os.path.join(class_folder, img_name))\n",
    "                test_labels.append(class_to_idx[class_name])\n",
    "print(f\"Test images: {len(test_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Base 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTWithHead(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_classes, dropout_rate=0.4):\n",
    "        super(ViTWithHead, self).__init__()\n",
    "        self.vit = pretrained_model  # Base ViT model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.vit.config.hidden_size),\n",
    "            nn.Dropout(dropout_rate), \n",
    "            nn.Linear(self.vit.config.hidden_size, 256), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.3), \n",
    "            nn.Linear(256, num_classes)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(x) \n",
    "        cls_token = outputs.last_hidden_state[:, 0]  # CLS token (first token)\n",
    "        logits = self.classifier(cls_token) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetunedModelPath = \"Models/ViT/Finetuned/ViT-Base16-in21k.pth\"\n",
    "modelPath = \"Models/ViT/Not-Finetuned/ViT-Base16-in21k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(finetunedModelPath, weights_only=False)\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTWithHead(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.4, inplace=False)\n",
       "    (2): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_paths, train_labels, processor, model, device)\n",
    "val_dataset = CustomDataset(val_paths, val_labels, processor, model, device)\n",
    "test_dataset = CustomDataset(test_paths, test_labels, processor, model, device)\n",
    "\n",
    "trainLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validLoader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "testLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.74      0.73      0.73       179\n",
      "  Osteopenia       0.82      0.86      0.84       180\n",
      "Osteoporosis       0.85      0.82      0.84       177\n",
      "\n",
      "    accuracy                           0.80       536\n",
      "   macro avg       0.80      0.80      0.80       536\n",
      "weighted avg       0.80      0.80      0.80       536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels, preds = evaluate_model_pytorch(model, testLoader, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold for class 0: 0.3983\n",
      "Best threshold for class 1: 0.6438\n",
      "Best threshold for class 2: 0.1324\n",
      "Classification Report with Optimized Thresholds:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.72      0.73      0.72       179\n",
      "  Osteopenia       0.83      0.82      0.82       180\n",
      "Osteoporosis       0.83      0.83      0.83       177\n",
      "\n",
      "    accuracy                           0.79       536\n",
      "   macro avg       0.79      0.79      0.79       536\n",
      "weighted avg       0.79      0.79      0.79       536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels, preds = evaluate_with_best_thresholds_pytorch(model, testLoader, categories, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
