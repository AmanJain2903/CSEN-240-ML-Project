# -*- coding: utf-8 -*-
"""BestThresholds.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W6Gbxcopd4O-Ye5-VKoyK7g7HnPKX7lZ

# IMPORTS
"""

import tensorflow as tf
import numpy as np
import os
from tensorflow.keras.metrics import Precision, Recall, AUC
from sklearn.metrics import f1_score, precision_recall_curve
import json

"""# DATA PREPARATION"""

dataPath = 'Dataset/'
categories = ["Normal","Osteopenia", "Osteoporosis"]

img_size = (224, 224)
channels = 3
img_shape = (img_size[0], img_size[1], channels)
batch_size = 16

trainDirectory = dataPath + "train"
testDirectory = dataPath + "test"
valDirectory = dataPath + "val"

trainDataset = tf.keras.utils.image_dataset_from_directory(
    trainDirectory,
    labels='inferred',
    label_mode='categorical',
    class_names=categories,
    color_mode='rgb',
    image_size=img_size,
    shuffle=True,
    seed=99,
    validation_split=None,
    batch_size=None,
    subset=None,
)

validDataset = tf.keras.utils.image_dataset_from_directory(
    valDirectory,
    labels='inferred',
    label_mode='categorical',
    class_names=categories,
    color_mode='rgb',
    image_size=img_size,
    shuffle=True,
    seed=99,
    validation_split=None,
    batch_size=None,
    subset=None,
)

testDataset = tf.keras.utils.image_dataset_from_directory(
    testDirectory,
    labels='inferred',
    label_mode='categorical',
    class_names=categories,
    color_mode='rgb',
    image_size=img_size,
    shuffle=True,
    seed=99,
    validation_split=None,
    batch_size=None,
    subset=None,
)

def normalize(image, label):
    image = tf.image.convert_image_dtype(image, tf.float32) / 255.0
    return image, label

trainDataset = trainDataset.map(normalize)
validDataset = validDataset.map(normalize)
testDataset = testDataset.map(normalize)

trainDataset = trainDataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)
validDataset = validDataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)
testDataset = testDataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)

"""# Get Best Thresholds Function"""

def bestThresholds(model, valDataset):
    valid_labels = []
    for xBatch, yBatch in valDataset:
        valid_labels.extend(np.argmax(yBatch, axis=1))
    predictions = model.predict(valDataset)
    yTrue = np.eye(3)[valid_labels]
    yPred = predictions
    thresholds = {}

    for class_idx in range(3):
        precision, recall, thresholds_pr = precision_recall_curve(yTrue[:, class_idx], yPred[:, class_idx])

        f1_scores = 2 * (precision * recall) / (precision + recall)

        optimal_threshold_pr = thresholds_pr[np.argmax(f1_scores)]
        thresholds[class_idx] = optimal_threshold_pr

    return thresholds

"""# Get Best Thresholds for All Models"""

def convert_float32(obj):
    if isinstance(obj, dict):
        return {key: convert_float32(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_float32(item) for item in obj]
    elif isinstance(obj, np.float32):
        return float(obj)  # Convert numpy.float32 to native Python float
    else:
        return obj

kerasModelsPath = 'KerasModels/'
modelNames = os.listdir(kerasModelsPath)
print("Models found: ", modelNames)

for modelName in modelNames:
    name = modelName.split('.')[0]
    print("Processing: ", name)
    model = tf.keras.models.load_model(kerasModelsPath + modelName)
    bestThresholdsDict = bestThresholds(model, validDataset)
    bestThresholdsDict = convert_float32(bestThresholdsDict)
    with open('Thresholds/' + name + '.json', 'w') as jsonFile:
        json.dump(bestThresholdsDict, jsonFile)